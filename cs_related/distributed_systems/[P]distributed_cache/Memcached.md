# Scaling Memcache at Facebook 阅读笔记
## 简介
这篇笔记是对于https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final170_update.pdf

这篇文档的阅读记录，文中按照“延迟和负载” “区域内机制” “区域间机制”等三个方面对于Facebook的缓存系统进行了一个介绍，但是内容的排布让人难以一下理解其系统的原本面貌，在此做读书笔记，按照自己的理解对于文中的内容和机制进行整理，以期一览其大概

## 总览
[图1]
上图是我在读了文章之后对于整个缓存系统的理解，其中每个标号为后续会讨论到的步骤：
+ FaceBook的基本架构，Front-End集群
+ 在同一个Region中的数据分布
+ 在web server数据存取的具体操作
+ mcrouter的工作模式
+ Memcache  pool是如何组织的，怎么分
+ 处理在缓存查询期间发生的错误的问题
+ region之间的memcached server share的作用
+ 冷集群启动的热集群依附
+ 读写本地DB的时候是如何使得缓存值更新或者失效的
+ 按照区域划分的时候写的是从库怎么办 remote marker机制的作用

### FaceBook web系统的基本架构

    首先在第一个部分描述一个场景前提，FaceBook这个项目其实和微博或者是朋友圈的项目有非常大的类似之处，就是读请求远远大于产生的写请求，并且写请求的生效实时性并不是要求很高，即使发布一个心情或者消息在几秒之后才能生效也是一个在容忍范围内的故事。

    作为FaceBook这么打体量的一个全球性提供服务的公司来说，必然会在世界各地都部署足够的服务，以便于用户就近访问数据，所以整个项目的架构基础就是按照区域进行部署web server

    在按照区域进行web server部署的这么一个大前提下，为了能够让web server提供足够快的服务，自然而然缓存和DB数据也都是要跟着web server进行全球部署，所以在整个FaceBook声称的区域Region中包含以下三个基本内容：

+ Web server (web 服务)
+ Memche (缓存服务)
+ DB (数据库服务)


   其中，一部分的web server和一部分的memche则组成一个可能以数据中心也可能是机房为单位的这么一个Front-End Cluster面向用户提供服务，而多个同一区域的Front-END cluster访问同一组的DB存储集群服务
   
 ### Region中的数据分布
      在图中可以看到，数据被存储在多个DB中，并且有多个memche进行缓存，所以其中的数据必然是按照一定的规则进行了分布式的存储，不然每个一个服务能够单独承载这个级别的数据。

     而其数据分布存储的规则则是由提供给web server使用的client依赖mcrouter，在client中提供数据分片，请求合并，容错超时控制等等功能

     但是，由于缓存数据往往存在热点的问题，并且存在一部分冷数据的问题，所以在缓存中，其数据并非像传统的分库分表那样。在缓存中部分热数据为了降低负载，会将负载分发到更多机器上，而部分冷数据可能由于数据使用量过小，可以让多个同一Region内的Front-Cluster来分享同一个memche server


### Web server的数据存取方式
基础对于web server的大概缓存使用方式和其他地方的并无差别，基础逻辑如下图所示
[图2]
读请求：
+ 优先读缓存
+ 缓存无数据则读DB
+ 读DB之后将新值更新给缓存

写请求：
+ 直接写DB
+ 从缓存中把DB的key进行删除

在大致的逻辑明确的基础上，FaceBook对于其中的几个进行了一些特殊的处理

### 平行请求以及请求合并
[图3]
     说白了就是对于可以并行获取的数据进行尽量的并行，并且为了防止过多的数据交互，尽量使得每个数据包承载的数据量趋于合理，将多个小请求进行合并成一个，降低网络交互的次数和可能由此引起的等待时间
	 
	 
### 基于UDP和TCP混合使用的网络交互
     由于 TCP连接和UDP连接使用上存在较大的差别，TCP连接需要三次握手才能建立连接，之后再通过网络包进行数据的交互和查询，但是UDP连接直接能够省略其连接建立的过程

     在一般的项目中，TCP连接自带的校验以及确保传输的功能非常有用，并且三次握手的消耗也基本感觉不到，所以TCP在一般网络项目中比较常见

     Facebook这个缓存项目却有着不一样的看法，因为缓存的目的本身就是为了更快的提供服务，所以UDP协议能更好的提供“快”这一属性，因为对于这个服务来说，确保传输的缓慢网络远远比不上大部分能功能顺利的高速交互

     最终，FaceBook采用了以下图中的交互方式，查询的部分走UDP提高速度，而具体有传输质量需求的部分使用TCP进行交互
[图4]
     并且针对UDP不可靠交互的使用新增了如下的规则：
***当UDP交互超过时间没能获取应答时，视作获取缓存失败，从DB获取新数据，但之后跳过更新缓存信息的步骤***

### 基于滑动窗口的流量控制
     TCP协议除了能提供确保包传输之外，还能提供一系列其他功能，包括包验证、基于滑动窗口的连接流量控制等等。

     FaceBook使用UDP进行数据交互的时候只能自己设计了一套基于UDP协议的滑动窗口机制来确保对于流量进行控制

     滑动窗口的基本原理并非什么独创的技术，TCP的原本实现中就有现成的案例，在这里就不进行展开了
	 
### Mcrouter的工作模式

上文中有提及Mcrouter是作为web server连接memcache的一个客户端进行使用，并且当web server需要查询特定key值的时候，Mcrouter可以对于当前key值进行计算，知晓具体去哪个或者哪几个memcache中获取数据
[图5]
但是mcrouter和memcache还是存在着一些交互上的问题，特别是并发这个因素被引入系统之后，比较大的问题为以下两个

### 旧值覆盖
     当存在如下这种情况时，有web server 1,2同时查询值A,并有其他请求实时更新A的值，致使A的值从3变成4，而server 1,2正好分别获取到一个值的两种状态3和4



     根据上文中所属的缓存未命中的策略，A的值将被set到memcache中，以便于后续的缓存命中
 [图6]


     但是由于两个时并行进行的，并且网络或者时机器边界上的数据我们没法控制其顺序，所以在memcache中的A的最终缓存值是不稳定的，并且由于后续的其他数据请求都能在memcache中查询到A的值，不论到底是多少，所以其他后续的请求不会去DB中校验A值的正确性

     而我们可以知道，在这个场景中只有A的值一直是4才是符合正确这一要求的，也就是由于并发的关系A=3这个比较旧的状态可能会被覆盖到缓存中A=4的状态，导致后续的请求获取到的A都是3这个不正确的数据
	 
### Thundering herd
     这个相对比较好理解，因为有时候单个热点数据会被各个方向的请求来回读写，造成了在缓存中热点数据被反复修改为无效，大量请求的数据获取击穿缓存直接将压力穿透到DB上，可能造成DB宕机或者连接超限等等灾难性后果，并且由于faceBook以及类似微博这样的服务上的数据特性，这个场景并非少见。
	 